,id,title,author,journal,year,volume,ref,pages,booktitle
0,@articleSzegedy2014IntriguingPO,Intriguing properties of neural networks,"['Christian Szegedy', 'W. Zaremba', 'Ilya Sutskever', 'Joan Bruna', 'D. Erhan', 'Ian J. Goodfellow', 'R. Fergus']",CoRR,2014,abs/1312.6199,"Szegedy, Christian et al. “Intriguing properties of neural networks.” CoRR abs/1312.6199 (2014): n. pag.",,
1,@articleGoodfellow2015ExplainingAH,Explaining and Harnessing Adversarial Examples,"['Ian J. Goodfellow', 'Jonathon Shlens', 'Christian Szegedy']",CoRR,2015,abs/1412.6572,"Goodfellow, Ian J. et al. “Explaining and Harnessing Adversarial Examples.” CoRR abs/1412.6572 (2015): n. pag.",,
2,@articleCarlini2017TowardsET,Towards Evaluating the Robustness of Neural Networks,"['Nicholas Carlini', 'D. Wagner']",2017 IEEE Symposium on Security and Privacy (SP),2017,,"Carlini, Nicholas and D. Wagner. “Towards Evaluating the Robustness of Neural Networks.” 2017 IEEE Symposium on Security and Privacy (SP) (2017): 39-57.",39-57,
3,@articleKurakin2017AdversarialML,Adversarial Machine Learning at Scale,"['A. Kurakin', 'Ian J. Goodfellow', 'S. Bengio']",ArXiv,2017,abs/1611.01236,"Kurakin, A. et al. “Adversarial Machine Learning at Scale.” ArXiv abs/1611.01236 (2017): n. pag.",,
4,@articlePapernot2018SoKSA,SoK: Security and Privacy in Machine Learning,"['Nicolas Papernot', 'P. McDaniel', 'Arunesh Sinha', 'Michael P. Wellman']",2018 IEEE European Symposium on Security and Privacy (EuroS&P),2018,,"Papernot, Nicolas et al. “SoK: Security and Privacy in Machine Learning.” 2018 IEEE European Symposium on Security and Privacy (EuroS&P) (2018): 399-414.",399-414,
5,@articleTramr2018EnsembleAT,Ensemble Adversarial Training: Attacks and Defenses,"['Florian Tram\\`er', 'A. Kurakin', 'Nicolas Papernot', 'D. Boneh', 'P. McDaniel']",ArXiv,2018,abs/1705.07204,"Tramèr, Florian et al. “Ensemble Adversarial Training: Attacks and Defenses.” ArXiv abs/1705.07204 (2018): n. pag.",,
6,@inproceedingsBuckman2018ThermometerEO,Thermometer Encoding: One Hot Way To Resist Adversarial Examples,"['J. Buckman', 'Aurko Roy', 'Colin Raffel', 'Ian J. Goodfellow']",,2018,,"Buckman, J. et al. “Thermometer Encoding: One Hot Way To Resist Adversarial Examples.” ICLR (2018).",,ICLR
7,@articleDhillon2018StochasticAP,Stochastic Activation Pruning for Robust Adversarial Defense,"['Guneet S. Dhillon', 'Kamyar Azizzadenesheli', 'Zachary Chase Lipton', 'J. Bernstein', 'Jean Kossaifi', 'A. Khanna', 'Anima An', 'kumar']",ArXiv,2018,abs/1803.01442,"Dhillon, Guneet S. et al. “Stochastic Activation Pruning for Robust Adversarial Defense.” ArXiv abs/1803.01442 (2018): n. pag.",,
8,@articleSamangouei2018DefenseGANPC,Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models,"['Pouya Samangouei', 'Maya Kabkab', 'R. Chellappa']",ArXiv,2018,abs/1805.06605,"Samangouei, Pouya et al. “Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models.” ArXiv abs/1805.06605 (2018): n. pag.",,
9,@articleGuo2018CounteringAI,Countering Adversarial Images using Input Transformations,"['Chuan Guo', 'Mayank Rana', ""M. Ciss\\'e"", 'L. V. D. Maaten']",ArXiv,2018,abs/1711.00117,"Guo, Chuan et al. “Countering Adversarial Images using Input Transformations.” ArXiv abs/1711.00117 (2018): n. pag.",,
10,@articleAthalye2018ObfuscatedGG,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,"['A. Athalye', 'Nicholas Carlini', 'D. Wagner']",ArXiv,2018,abs/1802.00420,"Athalye, A. et al. “Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples.” ArXiv abs/1802.00420 (2018): n. pag.",,
11,@inproceedingsGrosse2017AdversarialEF,Adversarial Examples for Malware Detection,"['Kathrin Grosse', 'Nicolas Papernot', 'P. Manoharan', 'M. Backes', 'P. McDaniel']",,2017,,"Grosse, Kathrin et al. “Adversarial Examples for Malware Detection.” ESORICS (2017).",,ESORICS
12,@articleZhao2018GeneratingNA,Generating Natural Adversarial Examples,"['Zhengli Zhao', 'Dheeru Dua', 'Sameer Singh']",ArXiv,2018,abs/1710.11342,"Zhao, Zhengli et al. “Generating Natural Adversarial Examples.” ArXiv abs/1710.11342 (2018): n. pag.",,
13,@articleYuan2019AdversarialEA,Adversarial Examples: Attacks and Defenses for Deep Learning,"['Xiaoyong Yuan', 'Pan He', 'Qile Zhu', 'X. Li']",IEEE Transactions on Neural Networks and Learning Systems,2019,30,"Yuan, Xiaoyong et al. “Adversarial Examples: Attacks and Defenses for Deep Learning.” IEEE Transactions on Neural Networks and Learning Systems 30 (2019): 2805-2824.",2805-2824,
14,@articleAkhtar2018ThreatOA,Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey,"['N. Akhtar', 'A. Mian']",IEEE Access,2018,6,"Akhtar, N. and A. Mian. “Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey.” IEEE Access 6 (2018): 14410-14430.",14410-14430,
15,@articleLiu2019DPATCHAA,DPATCH: An Adversarial Patch Attack on Object Detectors,"['Xin Liu', 'Huanrui Yang', 'Z. Liu', 'Linghao Song', 'Yiran Chen', 'Hai Li']",arXiv: Computer Vision and Pattern Recognition,2019,,"Liu, Xin et al. “DPATCH: An Adversarial Patch Attack on Object Detectors.” arXiv: Computer Vision and Pattern Recognition (2019): n. pag.",,
16,@articlePapernot2016TransferabilityIM,Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples,"['Nicolas Papernot', 'P. McDaniel', 'Ian J. Goodfellow']",ArXiv,2016,abs/1605.07277,"Papernot, Nicolas et al. “Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples.” ArXiv abs/1605.07277 (2016): n. pag.",,
17,@articleLiu2017DelvingIT,Delving into Transferable Adversarial Examples and Black-box Attacks,"['Y. Liu', 'X. Chen', 'Chang Liu', 'D. Song']",ArXiv,2017,abs/1611.02770,"Liu, Y. et al. “Delving into Transferable Adversarial Examples and Black-box Attacks.” ArXiv abs/1611.02770 (2017): n. pag.",,
18,@inproceedingsGoodfellow2013MaxoutN,Maxout Networks,"['Ian J. Goodfellow', 'David Warde-Farley', 'M. Mirza', 'Aaron C. Courville', 'Yoshua Bengio']",,2013,,"Goodfellow, Ian J. et al. “Maxout Networks.” ICML (2013).",,ICML
19,@articleKurakin2017AdversarialEI,Adversarial examples in the physical world,"['A. Kurakin', 'Ian J. Goodfellow', 'S. Bengio']",ArXiv,2017,abs/1607.02533,"Kurakin, A. et al. “Adversarial examples in the physical world.” ArXiv abs/1607.02533 (2017): n. pag.",,
20,@articleMoosaviDezfooli2016DeepFoolAS,DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks,"['Seyed-Mohsen Moosavi-Dezfooli', 'Alhussein Fawzi', 'P. Frossard']",2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2016,,"Moosavi-Dezfooli, Seyed-Mohsen et al. “DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016): 2574-2582.",2574-2582,
21,@articlePapernot2016TheLO,The Limitations of Deep Learning in Adversarial Settings,"['Nicolas Papernot', 'P. McDaniel', 'S. Jha', 'Matt Fredrikson', 'Z. Y. Celik', 'A. Swami']",2016 IEEE European Symposium on Security and Privacy (EuroS&P),2016,,"Papernot, Nicolas et al. “The Limitations of Deep Learning in Adversarial Settings.” 2016 IEEE European Symposium on Security and Privacy (EuroS&P) (2016): 372-387.",372-387,
22,@articleRozsa2016AdversarialDA,Adversarial Diversity and Hard Positive Generation,"['Andras Rozsa', 'Ethan M. Rudd', 'T. Boult']",2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2016,,"Rozsa, Andras et al. “Adversarial Diversity and Hard Positive Generation.” 2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2016): 410-417.",410-417,
23,@articleMoosaviDezfooli2017UniversalAP,Universal Adversarial Perturbations,"['Seyed-Mohsen Moosavi-Dezfooli', 'Alhussein Fawzi', 'Omar Fawzi', 'P. Frossard']",2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2017,,"Moosavi-Dezfooli, Seyed-Mohsen et al. “Universal Adversarial Perturbations.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017): 86-94.",86-94,
24,@articleKos2018AdversarialEF,Adversarial Examples for Generative Models,"['J. Kos', 'I. Fischer', 'D. Song']",2018 IEEE Security and Privacy Workshops (SPW),2018,,"Kos, J. et al. “Adversarial Examples for Generative Models.” 2018 IEEE Security and Privacy Workshops (SPW) (2018): 36-42.",36-42,
25,@inproceedingsBaluja2018LearningTA,Learning to Attack: Adversarial Transformation Networks,"['S. Baluja', 'Ian S. Fischer']",,2018,,"Baluja, S. and Ian S. Fischer. “Learning to Attack: Adversarial Transformation Networks.” AAAI (2018).",,AAAI
26,@articleXie2017AdversarialEF,Adversarial Examples for Semantic Segmentation and Object Detection,"['Cihang Xie', 'Jianyu Wang', 'Zhishuai Zhang', 'Yuyin Zhou', 'Lingxi Xie', 'A. Yuille']",2017 IEEE International Conference on Computer Vision (ICCV),2017,,"Xie, Cihang et al. “Adversarial Examples for Semantic Segmentation and Object Detection.” 2017 IEEE International Conference on Computer Vision (ICCV) (2017): 1378-1387.",1378-1387,
27,@articleChen2017ZOOZO,ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models,"['P. Chen', 'Huan Zhang', 'Yash Sharma', 'Jinfeng Yi', 'Cho-Jui Hsieh']",Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security,2017,,"Chen, P. et al. “ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models.” Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (2017): n. pag.",,
28,@articleSu2019OnePA,One Pixel Attack for Fooling Deep Neural Networks,"['Jiawei Su', 'Danilo Vasconcellos Vargas', 'K. Sakurai']",IEEE Transactions on Evolutionary Computation,2019,23,"Su, Jiawei et al. “One Pixel Attack for Fooling Deep Neural Networks.” IEEE Transactions on Evolutionary Computation 23 (2019): 828-841.",828-841,
29,@articleDong2018BoostingAA,Boosting Adversarial Attacks with Momentum,"['Yinpeng Dong', 'Fangzhou Liao', 'Tianyu Pang', 'H. Su', 'J. Zhu', 'Xiaolin Hu', 'J. Li']",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,2018,,"Dong, Yinpeng et al. “Boosting Adversarial Attacks with Momentum.” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018): 9185-9193.",9185-9193,
30,@articleXiao2018GeneratingAE,Generating Adversarial Examples with Adversarial Networks,"['Chaowei Xiao', 'Bo Li', 'Jun-Yan Zhu', 'Warren He', 'M. Liu', 'D. Song']",ArXiv,2018,abs/1801.02610,"Xiao, Chaowei et al. “Generating Adversarial Examples with Adversarial Networks.” ArXiv abs/1801.02610 (2018): n. pag.",,
31,@articleBrendel2018DecisionBasedAA,Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models,"['W. Brendel', 'Jonas Rauber', 'M. Bethge']",ArXiv,2018,abs/1712.04248,"Brendel, W. et al. “Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models.” ArXiv abs/1712.04248 (2018): n. pag.",,
32,@articleXiao2018SpatiallyTA,Spatially Transformed Adversarial Examples,"['Chaowei Xiao', 'Jun-Yan Zhu', 'Bo Li', 'Warren He', 'M. Liu', 'D. Song']",ArXiv,2018,abs/1801.02612,"Xiao, Chaowei et al. “Spatially Transformed Adversarial Examples.” ArXiv abs/1801.02612 (2018): n. pag.",,
33,@articleAthalye2018SynthesizingRA,Synthesizing Robust Adversarial Examples,"['A. Athalye', 'L. Engstrom', 'Andrew Ilyas', 'Kevin Kwok']",ArXiv,2018,abs/1707.07397,"Athalye, A. et al. “Synthesizing Robust Adversarial Examples.” ArXiv abs/1707.07397 (2018): n. pag.",,
34,@articleUesato2018AdversarialRA,Adversarial Risk and the Dangers of Evaluating Against Weak Attacks,"['Jonathan Uesato', ""Brendan O'Donoghue"", 'A. Oord', 'Pushmeet Kohli']",ArXiv,2018,abs/1802.05666,"Uesato, Jonathan et al. “Adversarial Risk and the Dangers of Evaluating Against Weak Attacks.” ArXiv abs/1802.05666 (2018): n. pag.",,
35,@articleHuang2017AdversarialAO,Adversarial Attacks on Neural Network Policies,"['S', 'y H. Huang', 'Nicolas Papernot', 'Ian J. Goodfellow', 'Yan Duan', 'P. Abbeel']",ArXiv,2017,abs/1702.02284,"Huang, Sandy H. et al. “Adversarial Attacks on Neural Network Policies.” ArXiv abs/1702.02284 (2017): n. pag.",,
36,@articleFischer2017AdversarialEF,Adversarial Examples for Semantic Image Segmentation,"['Volker Fischer', 'Mummadi Chaithanya Kumar', 'J. H. Metzen', 'T. Brox']",ArXiv,2017,abs/1703.01101,"Fischer, Volker et al. “Adversarial Examples for Semantic Image Segmentation.” ArXiv abs/1703.01101 (2017): n. pag.",,
37,@articleKurakin2018AdversarialAA,Adversarial Attacks and Defences Competition,"['A. Kurakin', 'Ian J. Goodfellow', 'S. Bengio', 'Yinpeng Dong', 'Fangzhou Liao', 'M. Liang', 'Tianyu Pang', 'Jun Zhu', 'Xiaolin Hu', 'Cihang Xie', 'Jianyu Wang', 'Zhishuai Zhang', 'Zhou Ren', 'A. Yuille', 'Sangxia Huang', 'Y. Zhao', 'Zhonglin Han', 'J. Long', 'Yerkebulan Berdibekov', 'Takuya Akiba', 'Seiya Tokui', 'M. Abe']",ArXiv,2018,abs/1804.00097,"Kurakin, A. et al. “Adversarial Attacks and Defences Competition.” ArXiv abs/1804.00097 (2018): n. pag.",,
38,@inproceedingsArjovsky2017WassersteinGA,Wasserstein Generative Adversarial Networks,"[""Mart\\'in Arjovsky"", 'Soumith Chintala', 'L. Bottou']",,2017,,"Arjovsky, Martín et al. “Wasserstein Generative Adversarial Networks.” ICML (2017).",,ICML
39,@inproceedingsChen2018RobustPA,Robust Physical Adversarial Attack on Faster R-CNN Object Detector,"['Shang-Tse Chen', 'Cory Cornelius', 'J. Martin', 'Duen Horng Chau']",,2018,,"Chen, Shang-Tse et al. “Robust Physical Adversarial Attack on Faster R-CNN Object Detector.” ECML/PKDD (2018).",,ECML/PKDD
40,@articleLiao2018DefenseAA,Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser,"['Fangzhou Liao', 'Ming Liang', 'Yinpeng Dong', 'Tianyu Pang', 'J. Zhu', 'Xiaolin Hu']",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,2018,,"Liao, Fangzhou et al. “Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser.” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018): 1778-1787.",1778-1787,
41,@articleSaadatpanah2019AdversarialAO,Adversarial attacks on Copyright Detection Systems,"['Parsa Saadatpanah', 'A. Shafahi', 'T. Goldstein']",ArXiv,2019,abs/1906.07153,"Saadatpanah, Parsa et al. “Adversarial attacks on Copyright Detection Systems.” ArXiv abs/1906.07153 (2019): n. pag.",,
42,@articleHaenssle2018ManAM,Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists,"['H. Haenssle', 'C. Fink', 'R. Schneiderbauer', 'F. Toberer', 'T. Buhl', 'A. Blum', 'A. Kalloo', 'A. Hassen', 'L. Thomas', 'A. Enk', 'L. Uhlmann']",Annals of Oncology,2018,29,"Haenssle, H. et al. “Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists.” Annals of Oncology 29 (2018): 1836–1842.",1836–1842,
43,@articleFinlayson2018AdversarialAA,Adversarial Attacks Against Medical Deep Learning Systems,"['Samuel G. Finlayson', 'I. Kohane', 'Andrew Beam']",ArXiv,2018,abs/1804.05296,"Finlayson, Samuel G. et al. “Adversarial Attacks Against Medical Deep Learning Systems.” ArXiv abs/1804.05296 (2018): n. pag.",,
44,@articleGu2015TowardsDN,Towards Deep Neural Network Architectures Robust to Adversarial Examples,"['Shixiang Gu', 'Luca Rigazio']",CoRR,2015,abs/1412.5068,"Gu, Shixiang and Luca Rigazio. “Towards Deep Neural Network Architectures Robust to Adversarial Examples.” CoRR abs/1412.5068 (2015): n. pag.",,
45,@articleCarlini2016DefensiveDI,Defensive Distillation is Not Robust to Adversarial Examples,"['Nicholas Carlini', 'D. Wagner']",ArXiv,2016,abs/1607.04311,"Carlini, Nicholas and D. Wagner. “Defensive Distillation is Not Robust to Adversarial Examples.” ArXiv abs/1607.04311 (2016): n. pag.",,
46,@articleKannan2018AdversarialLP,Adversarial Logit Pairing,"['Harini Kannan', 'A. Kurakin', 'Ian J. Goodfellow']",ArXiv,2018,abs/1803.06373,"Kannan, Harini et al. “Adversarial Logit Pairing.” ArXiv abs/1803.06373 (2018): n. pag.",,
47,@articleLamb2018FortifiedNI,Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations,"['Alex Lamb', 'Jonathan Binas', 'Anirudh Goyal', 'Dmitriy Serdyuk', 'S', 'eep Subramanian', 'Ioannis Mitliagkas', 'Yoshua Bengio']",ArXiv,2018,abs/1804.02485,"Lamb, Alex et al. “Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations.” ArXiv abs/1804.02485 (2018): n. pag.",,
48,@articleSchott2019TowardsTF,Towards the first adversarially robust neural network model on MNIST,"['Lukas Schott', 'Jonas Rauber', 'M. Bethge', 'W. Brendel']",arXiv: Computer Vision and Pattern Recognition,2019,,"Schott, Lukas et al. “Towards the first adversarially robust neural network model on MNIST.” arXiv: Computer Vision and Pattern Recognition (2019): n. pag.",,
49,@articleDubey2019DefenseAA,Defense Against Adversarial Images Using Web-Scale Nearest-Neighbor Search,"['A. Dubey', 'L. V. D. Maaten', 'Zeki Yalniz', 'Y. Li', 'D. Mahajan']",2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2019,,"Dubey, A. et al. “Defense Against Adversarial Images Using Web-Scale Nearest-Neighbor Search.” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019): 8759-8768.",8759-8768,
50,@articleYang2019MENetTE,ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation,"['Yuzhe Yang', 'G. Zhang', 'D. Katabi', 'Zhi Xu']",ArXiv,2019,abs/1905.11971,"Yang, Yuzhe et al. “ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation.” ArXiv abs/1905.11971 (2019): n. pag.",,
51,@articleMetzen2017OnDA,On Detecting Adversarial Perturbations,"['J. H. Metzen', 'Tim Genewein', 'Volker Fischer', 'B. Bischoff']",ArXiv,2017,abs/1702.04267,"Metzen, J. H. et al. “On Detecting Adversarial Perturbations.” ArXiv abs/1702.04267 (2017): n. pag.",,
52,@articleGong2017AdversarialAC,Adversarial and Clean Data Are Not Twins,"['Zhitao Gong', 'Wenlu Wang', 'W. Ku']",ArXiv,2017,abs/1704.04960,"Gong, Zhitao et al. “Adversarial and Clean Data Are Not Twins.” ArXiv abs/1704.04960 (2017): n. pag.",,
53,@articleHosseini2017BlockingTO,Blocking Transferability of Adversarial Examples in Black-Box Learning Systems,"['H. Hosseini', 'Yize Chen', 'S. Kannan', 'B. Zhang', 'R. Poovendran']",ArXiv,2017,abs/1703.04318,"Hosseini, H. et al. “Blocking Transferability of Adversarial Examples in Black-Box Learning Systems.” ArXiv abs/1703.04318 (2017): n. pag.",,
54,@articleFeinman2017DetectingAS,Detecting Adversarial Samples from Artifacts,"['Reuben Feinman', 'Ryan R. Curtin', 'S. Shintre', 'Andrew B. Gardner']",ArXiv,2017,abs/1703.00410,"Feinman, Reuben et al. “Detecting Adversarial Samples from Artifacts.” ArXiv abs/1703.00410 (2017): n. pag.",,
55,@articleXu2018FeatureSD,Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks,"['Weilin Xu', 'David Evans', 'Y. Qi']",ArXiv,2018,abs/1704.01155,"Xu, Weilin et al. “Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks.” ArXiv abs/1704.01155 (2018): n. pag.",,
56,@articleShaham2018UnderstandingAT,Understanding adversarial training: Increasing local stability of supervised models through robust optimization,"['Uri Shaham', 'Yutaro Yamada', 'Sah', 'N. Negahban']",Neurocomputing,2018,307,"Shaham, Uri et al. “Understanding adversarial training: Increasing local stability of supervised models through robust optimization.” Neurocomputing 307 (2018): 195-204.",195-204,
57,@articleEsfahani2018DatadrivenDR,Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations,"['Peyman Mohajerin Esfahani', 'D. Kuhn']",Mathematical Programming,2018,171,"Esfahani, Peyman Mohajerin and D. Kuhn. “Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations.” Mathematical Programming 171 (2018): 115-166.",115-166,
58,@articleTsipras2019RobustnessMB,Robustness May Be at Odds with Accuracy,"['D. Tsipras', 'Shibani Santurkar', 'L. Engstrom', 'A. Turner', 'A. Madry']",arXiv: Machine Learning,2019,,"Tsipras, D. et al. “Robustness May Be at Odds with Accuracy.” arXiv: Machine Learning (2019): n. pag.",,
59,@articleEngstrom2019LearningPR,Learning Perceptually-Aligned Representations via Adversarial Robustness,"['L. Engstrom', 'Andrew Ilyas', 'Shibani Santurkar', 'D. Tsipras', 'B. Tran', 'A. Madry']",ArXiv,2019,abs/1906.00945,"Engstrom, L. et al. “Learning Perceptually-Aligned Representations via Adversarial Robustness.” ArXiv abs/1906.00945 (2019): n. pag.",,
60,@articleTramr2019AdversarialTA,Adversarial Training and Robustness for Multiple Perturbations,"['Florian Tram\\`er', 'D. Boneh']",ArXiv,2019,abs/1904.13000,"Tramèr, Florian and D. Boneh. “Adversarial Training and Robustness for Multiple Perturbations.” ArXiv abs/1904.13000 (2019): n. pag.",,
61,@articleKang2019TransferOA,Transfer of Adversarial Robustness Between Perturbation Types,"['Daniel Kang', 'Y. Sun', 'Tom Brown', 'Dan Hendrycks', 'J. Steinhardt']",ArXiv,2019,abs/1905.01034,"Kang, Daniel et al. “Transfer of Adversarial Robustness Between Perturbation Types.” ArXiv abs/1905.01034 (2019): n. pag.",,
62,@articleDziugaite2016ASO,A study of the effect of JPG compression on adversarial images,"['G. Dziugaite', 'Zoubin Ghahramani', 'D. Roy']",ArXiv,2016,abs/1608.00853,"Dziugaite, G. et al. “A study of the effect of JPG compression on adversarial images.” ArXiv abs/1608.00853 (2016): n. pag.",,
63,@inproceedingsKolter2018ProvableDA,Provable defenses against adversarial examples via the convex outer adversarial polytope,"['J. Z. Kolter', 'E. Wong']",,2018,,"Kolter, J. Z. and E. Wong. “Provable defenses against adversarial examples via the convex outer adversarial polytope.” ICML (2018).",,ICML
64,@articleRaghunathan2018CertifiedDA,Certified Defenses against Adversarial Examples,"['Aditi Raghunathan', 'J. Steinhardt', 'Percy Liang']",ArXiv,2018,abs/1801.09344,"Raghunathan, Aditi et al. “Certified Defenses against Adversarial Examples.” ArXiv abs/1801.09344 (2018): n. pag.",,
65,@inproceedingsVincent2008ExtractingAC,Extracting and composing robust features with denoising autoencoders,"['Pascal Vincent', 'H. Larochelle', 'Yoshua Bengio', 'Pierre-Antoine Manzagol']",,2008,,"Vincent, Pascal et al. “Extracting and composing robust features with denoising autoencoders.” ICML '08 (2008).",,ICML '08
66,@articleCarlini2017AdversarialEA,Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods,"['N. Carlini', 'D. Wagner']",Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security,2017,,"Carlini, N. and D. Wagner. “Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods.” Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (2017): n. pag.",,
67,@articleTramr2017TheSO,The Space of Transferable Adversarial Examples,"['Florian Tram\\`er', 'Nicolas Papernot', 'Ian J. Goodfellow', 'D. Boneh', 'P. McDaniel']",ArXiv,2017,abs/1704.03453,"Tramèr, Florian et al. “The Space of Transferable Adversarial Examples.” ArXiv abs/1704.03453 (2017): n. pag.",,
68,@articleEtmann2019OnTC,On the Connection Between Adversarial Robustness and Saliency Map Interpretability,"['Christian Etmann', 'S. Lunz', 'P. Maass', 'C. Sch\\""onlieb']",ArXiv,2019,abs/1905.04172,"Etmann, Christian et al. “On the Connection Between Adversarial Robustness and Saliency Map Interpretability.” ArXiv abs/1905.04172 (2019): n. pag.",,
69,@articleGeirhos2019ImageNettrainedCA,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,"['Robert Geirhos', 'Patricia Rubisch', 'Claudio Michaelis', 'M. Bethge', 'F. Wichmann', 'W. Brendel']",ArXiv,2019,abs/1811.12231,"Geirhos, Robert et al. “ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.” ArXiv abs/1811.12231 (2019): n. pag.",,
70,@articleCarlini2019OnEA,On Evaluating Adversarial Robustness,"['Nicholas Carlini', 'A. Athalye', 'Nicolas Papernot', 'W. Brendel', 'Jonas Rauber', 'D. Tsipras', 'Ian J. Goodfellow', 'A. Madry', 'A. Kurakin']",ArXiv,2019,abs/1902.06705,"Carlini, Nicholas et al. “On Evaluating Adversarial Robustness.” ArXiv abs/1902.06705 (2019): n. pag.",,
71,@inproceedingsLiu2019BeyondPN,Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer,"['Hsueh-Ti Derek Liu', 'Michael Tao', 'C. Li', 'Derek Nowrouzezahrai', 'Alec Jacobson']",,2019,,"Liu, Hsueh-Ti Derek et al. “Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer.” ICLR (2019).",,ICLR
72,@articlePapernot2016DistillationAA,Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks,"['Nicolas Papernot', 'P. McDaniel', 'Xi Wu', 'S. Jha', 'A. Swami']",2016 IEEE Symposium on Security and Privacy (SP),2016,,"Papernot, Nicolas et al. “Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks.” 2016 IEEE Symposium on Security and Privacy (SP) (2016): 582-597.",582-597,
73,@articleMa2018CharacterizingAS,Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality,"['Xingjun Ma', 'Bo Li', 'Yisen Wang', 'S. Erfani', 'S. Wijewickrema', 'M. E. Houle', 'Grant Schoenebeck', 'D. Song', 'J. Bailey']",ArXiv,2018,abs/1801.02613,"Ma, Xingjun et al. “Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality.” ArXiv abs/1801.02613 (2018): n. pag.",,
74,@articleXie2018MitigatingAE,Mitigating adversarial effects through randomization,"['Cihang Xie', 'Jianyu Wang', 'Zhishuai Zhang', 'Zhou Ren', 'A. Yuille']",ArXiv,2018,abs/1711.01991,"Xie, Cihang et al. “Mitigating adversarial effects through randomization.” ArXiv abs/1711.01991 (2018): n. pag.",,
75,@articleSong2018PixelDefendLG,PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples,"['Yang Song', 'Taesup Kim', 'Sebastian Nowozin', 'S. Ermon', 'Nate Kushman']",ArXiv,2018,abs/1710.10766,"Song, Yang et al. “PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples.” ArXiv abs/1710.10766 (2018): n. pag.",,
76,@articleSharif2016AccessorizeTA,Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition,"['Mahmood Sharif', 'Sruti Bhagavatula', 'L. Bauer', 'M. Reiter']",Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security,2016,,"Sharif, Mahmood et al. “Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition.” Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (2016): n. pag.",,
77,@articlePapernot2016TransferabilityIM,Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples,"['Nicolas Papernot', 'P. McDaniel', 'Ian J. Goodfellow']",ArXiv,2016,abs/1605.07277,"Papernot, Nicolas et al. “Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples.” ArXiv abs/1605.07277 (2016): n. pag.",,
78,@articleHochreiter1998TheVG,The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions,['S. Hochreiter'],Int. J. Uncertain. Fuzziness Knowl. Based Syst.,1998,6,"Hochreiter, S.. “The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions.” Int. J. Uncertain. Fuzziness Knowl. Based Syst. 6 (1998): 107-116.",107-116,
79,@articleRony2019DecouplingDA,Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses,"[""J\\'er\\^ome Rony"", 'Luiz G. Hafemann', 'L. Oliveira', 'I. B. Ayed', 'R. Sabourin', 'Eric Granger']",2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2019,,"Rony, Jérôme et al. “Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses.” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019): 4317-4325.",4317-4325,
80,@articleEngstrom2018EvaluatingAU,Evaluating and Understanding the Robustness of Adversarial Logit Pairing,"['L. Engstrom', 'Andrew Ilyas', 'A. Athalye']",ArXiv,2018,abs/1807.10272,"Engstrom, L. et al. “Evaluating and Understanding the Robustness of Adversarial Logit Pairing.” ArXiv abs/1807.10272 (2018): n. pag.",,
81,@articlePang2017RobustDL,Robust Deep Learning via Reverse Cross-Entropy Training and Thresholding Test,"['T. Pang', 'Chao Du', 'J. Zhu']",ArXiv,2017,abs/1706.00633,"Pang, T. et al. “Robust Deep Learning via Reverse Cross-Entropy Training and Thresholding Test.” ArXiv abs/1706.00633 (2017): n. pag.",,
82,@articleHuang2015LearningWA,Learning with a Strong Adversary,"['Ruitong Huang', 'B. Xu', 'Dale Schuurmans', 'Csaba Szepesvari']",ArXiv,2015,abs/1511.03034,"Huang, Ruitong et al. “Learning with a Strong Adversary.” ArXiv abs/1511.03034 (2015): n. pag.",,
83,@articleSabour2016AdversarialMO,Adversarial Manipulation of Deep Representations,"['Sara Sabour', 'Yanshuai Cao', 'Fartash Faghri', 'David J. Fleet']",CoRR,2016,abs/1511.05122,"Sabour, Sara et al. “Adversarial Manipulation of Deep Representations.” CoRR abs/1511.05122 (2016): n. pag.",,
