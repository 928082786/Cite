,0
0,Intriguing properties of neural networks
1,Explaining and Harnessing Adversarial Examples
2,Towards Evaluating the Robustness of Neural Networks
3,Adversarial Machine Learning at Scale
4,SoK: Security and Privacy in Machine Learning
5,Ensemble Adversarial Training: Attacks and Defenses
6,Thermometer Encoding: One Hot Way To Resist Adversarial Examples
7,Stochastic Activation Pruning for Robust Adversarial Defense
8,Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models
9,Countering Adversarial Images using Input Transformations
10,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples
11,Adversarial Examples for Malware Detection
12,Generating Natural Adversarial Examples
13,Adversarial Examples: Attacks and Defenses for Deep Learning
14,Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey
15,DPATCH: An Adversarial Patch Attack on Object Detectors
16,Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples
17,Delving into Transferable Adversarial Examples and Black-box Attacks
18,Maxout Networks
19,Adversarial examples in the physical world
20,DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks
21,The Limitations of Deep Learning in Adversarial Settings
22,Adversarial Diversity and Hard Positive Generation
23,Universal Adversarial Perturbations
24,Adversarial Examples for Generative Models
25,Learning to Attack: Adversarial Transformation Networks
26,Adversarial Examples for Semantic Segmentation and Object Detection
27,ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models
28,One Pixel Attack for Fooling Deep Neural Networks
29,Boosting Adversarial Attacks with Momentum
30,Generating Adversarial Examples with Adversarial Networks
31,Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models
32,Spatially Transformed Adversarial Examples
33,Synthesizing Robust Adversarial Examples
34,Adversarial Risk and the Dangers of Evaluating Against Weak Attacks
35,Adversarial Attacks on Neural Network Policies
36,Adversarial Examples for Semantic Image Segmentation
37,Adversarial Attacks and Defences Competition
38,Wasserstein Generative Adversarial Networks
39,Robust Physical Adversarial Attack on Faster R-CNN Object Detector
40,Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser
41,Adversarial attacks on Copyright Detection Systems
42,Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists
43,Adversarial Attacks Against Medical Deep Learning Systems
44,Towards Deep Neural Network Architectures Robust to Adversarial Examples
45,Defensive Distillation is Not Robust to Adversarial Examples
46,Adversarial Logit Pairing
47,Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations
48,Towards the first adversarially robust neural network model on MNIST
49,Defense Against Adversarial Images Using Web-Scale Nearest-Neighbor Search
50,ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation
51,On Detecting Adversarial Perturbations
52,Adversarial and Clean Data Are Not Twins
53,Blocking Transferability of Adversarial Examples in Black-Box Learning Systems
54,Detecting Adversarial Samples from Artifacts
55,Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks
56,Understanding adversarial training: Increasing local stability of supervised models through robust optimization
57,Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations
58,Robustness May Be at Odds with Accuracy
59,Learning Perceptually-Aligned Representations via Adversarial Robustness
60,Adversarial Training and Robustness for Multiple Perturbations
61,Transfer of Adversarial Robustness Between Perturbation Types
62,A study of the effect of JPG compression on adversarial images
63,Provable defenses against adversarial examples via the convex outer adversarial polytope
64,Certified Defenses against Adversarial Examples
65,Extracting and composing robust features with denoising autoencoders
66,Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods
67,The Space of Transferable Adversarial Examples
68,On the Connection Between Adversarial Robustness and Saliency Map Interpretability
69,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness
70,On Evaluating Adversarial Robustness
71,Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer
72,Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks
73,Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality
74,Mitigating adversarial effects through randomization
75,PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples
76,Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition
77,The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions
78,Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses
79,Evaluating and Understanding the Robustness of Adversarial Logit Pairing
80,Robust Deep Learning via Reverse Cross-Entropy Training and Thresholding Test
81,Learning with a Strong Adversary
82,Adversarial Manipulation of Deep Representations
83,Measuring Invariances in Deep Networks
84,Visualizing Higher-Layer Features of a Deep Network
85,Learning Deep Architectures for AI
86,Building high-level features using large scale unsupervised learning
87,Visualizing and Understanding Convolutional Networks
88,ImageNet classification with deep convolutional neural networks
89,"A discriminatively trained, multiscale, deformable part model"
90,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation
91,Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups
92,Efficient Estimation of Word Representations in Vector Space
93,ImageNet: A large-scale hierarchical image database
94,How to Explain Individual Classification Decisions
95,"Severe Outcomes Among Patients with Coronavirus Disease 2019 (COVID-19) - United States, February 12-March 16, 2020."
96,The mnist database of handwritten digits
97,Visualizing and Understanding Convolutional Neural Networks
98,Neural Networks with Adaptive Activation Functions
99,Survey on Deep Neural Networks in Speech using Natural Language Processing
100,Dropout: a simple way to prevent neural networks from overfitting
101,Learning Multiple Layers of Features from Tiny Images
102,Deep neural networks are easily fooled: High confidence predictions for unrecognizable images
103,Going deeper with convolutions
104,Long Short-Term Memory
105,Large Scale Distributed Deep Networks
106,Multi-Prediction Deep Boltzmann Machines
107,Theano: A CPU and GPU Math Compiler in Python
108,Deep Residual Learning for Image Recognition
109,Adam: A Method for Stochastic Optimization
110,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
111,Gradient-based learning applied to document recognition
112,Rethinking the Inception Architecture for Computer Vision
113,Human-level control through deep reinforcement learning
114,Mastering the game of Go with deep neural networks and tree search
115,Distilling the Knowledge in a Neural Network
116,Speech recognition with deep recurrent neural networks
117,ImageNet Large Scale Visual Recognition Challenge
118,Anomaly detection: A survey
119,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
120,Striving for Simplicity: The All Convolutional Net
121,Safety Verification of Deep Neural Networks
122,Adversarial Perturbations Against Deep Neural Networks for Malware Classification
123,GradientBased Learning Applied to Document Recognition
124,Playing Atari with Deep Reinforcement Learning
125,Measuring Neural Net Robustness with Constraints
126,Rectifier Nonlinearities Improve Neural Network Acoustic Models
127,End to End Learning for Self-Driving Cars
128,Technical Report on the CleverHans v2.1.0 Adversarial Examples Library
129,Cleverhans V0.1: an Adversarial Machine Learning Library
130,All you need is a good init
131,Malware classification with recurrent networks
132,Large-scale malware classification using random projections and neural networks
133,The vermiform appendix impacts the risk of developing Parkinson’s disease
134,"Fast, Lean, and Accurate: Modeling Password Guessability Using Neural Networks"
135,Fractional Max-Pooling
136,Hidden Voice Commands
137,Globally Normalized Transition-Based Neural Networks
138,A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots
139,Droid-Sec: deep learning in android malware detection
140,On the Effectiveness of Defensive Distillation
141,Neural Networks in Mobile Robot Motion
142,1 Adversarial Perturbations of Deep Neural Networks
143,Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization
144,Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples
145,Evasion Attacks against Machine Learning at Test Time
146,Are Accuracy and Robustness Correlated
147,Adversarial Training Methods for Semi-Supervised Text Classification
148,Distributional Smoothing with Virtual Adversarial Training
149,Adversarial classification: An adversarial risk analysis approach
150,Virtual Adversarial Training for Semi-Supervised Text Classification
151,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"
152,Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS
153,Stealing Machine Learning Models via Prediction APIs
154,Practical Black-Box Attacks against Machine Learning
155,Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures
156,Privacy-preserving deep learning
157,Membership Inference Attacks Against Machine Learning Models
158,Can machine learning be secure?
159,ANTIDOTE: understanding and defending against poisoning of anomaly detectors
160,Certified Defenses for Data Poisoning Attacks
161,Learning in the presence of malicious errors
162,Is Feature Selection Secure against Training Data Poisoning?
163,Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers
164,Systematic Poisoning Attacks on and Defenses for Machine Learning in Healthcare
165,Differential Privacy: A Survey of Results
166,Generative Adversarial Nets
167,Deep Learning
168,Practical Evasion of a Learning-Based Classifier: A Case Study
169,The Algorithmic Foundations of Differential Privacy
170,Poisoning Attacks against Support Vector Machines
171,CryptoNets: applying neural networks to encrypted data with high throughput and accuracy
172,Online Anomaly Detection under Adversarial Impact
173,Differentially Private Empirical Risk Minimization
174,Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data
175,Censoring Representations with an Adversary
176,Deep Learning with Differential Privacy
177,LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks
178,Machine learning - a probabilistic perspective
179,"""Why Should I Trust You?"": Explaining the Predictions of Any Classifier"
180,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks
181,Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners
182,Support Vector Machines Under Adversarial Label Noise
183,No free lunch in data privacy
184,Oblivious Multi-Party Machine Learning on Trusted Processors
185,Security And Game Theory Algorithms Deployed Systems Lessons Learned
186,Honware: A Virtual Honeypot Framework for Capturing CPE and IoT Zero Days
187,Automatically Evading Classifiers: A Case Study on PDF Malware Classifiers
188,Optimal randomized classification in adversarial settings
189,Data Poisoning Attacks against Autoregressive Models
190,Several important problems in concrete production
191,What Can We Learn Privately?
192,Learning privately from multiparty data
193,ON DATA BANKS AND PRIVACY HOMOMORPHISMS
194,Learning Adversary Behavior in Security Games: A PAC Model Perspective
195,Big Data's Disparate Impact
196,Bounding an Attack ’ s Complexity for a Simple Learning Model
197,Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing
198,Mining time-changing data streams
199,Poisoning behavioral malware clustering
200,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response
201,From physical security to cybersecurity
202,Polygraph: automatically generating signatures for polymorphic worms
203,Understanding Black-box Predictions via Influence Functions
204,Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks
205,Misleading worm signature generators using deliberate noise injection
206,Sequence to Sequence Learning with Neural Networks
207,Static Analysis of Executables to Detect Malicious Patterns
208,Statistical fraud detection: A review
209,Query Strategies for Evading Convex-Inducing Classifiers
210,Stackelberg games for adversarial prediction problems
211,Fairness Constraints: Mechanisms for Fair Classification
212,Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds
213,Next Generation Intrusion Detection: Autonomous Reinforcement Learning of Network Attacks
214,Adversarial Label Flips Attack on Support Vector Machines
215,"ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection, Adversarial Examples and Model Criticism"
216,Reinforcement Learning: An Introduction
217,Dropout Inference in Bayesian Neural Networks with Alpha-divergences
218,Noise Tolerance Under Risk Minimization
219,Anomaly Based Network Intrusion Detection with Unsupervised Outlier Detection
220,Fairness through awareness
221,"Evaluation: from Precision, Recall and F-measure to ROC, Informedness, Markedness and Correlation"
222,A new learning paradigm: Learning using privileged information
223,Fair Algorithms for Machine Learning
224,Nightmare at test time: robust learning by feature deletion
225,Counterfactual Fairness in Text Classification through Robustness
226,Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization
227,"European Union Regulations on Algorithmic Decision-Making and a ""Right to Explanation"""
228,On the limited memory BFGS method for large scale optimization
229,"Privacy, information technology, and health care"
230,Large-Scale Machine Learning with Stochastic Gradient Descent
231,Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems
232,Preparing for the future of Artificial Intelligence
233,Supervision via competition: Robot adversaries for learning tasks
234,On Attacking Statistical Spam Filters
235,Estimation of the warfarin dose with clinical and pharmacogenetic data.
236,Nash Q-Learning for General-Sum Stochastic Games
237,An Introduction to Kernel and Nearest-Neighbor Nonparametric Regression
238,Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction
239,Algorithmic Decision Making and the Cost of Fairness
240,Auguste Kerckhoffs et la cryptographie militaire
241,Pattern Recognition Receptors and Inflammation
242,Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error
243,Why Does Unsupervised Pre-training Help Deep Learning?
244,Towards Deep Learning Models Resistant to Adversarial Attacks
245,Adversarial Transformation Networks: Learning to Generate Adversarial Examples
246,Improving Transferability of Adversarial Examples With Input Diversity
247,Feature Denoising for Improving Adversarial Robustness
248,Certifiable Distributional Robustness with Principled Adversarial Training
249,Foveation-based Mechanisms Alleviate Adversarial Examples
250,Towards the Science of Security and Privacy in Machine Learning
251,"Comment on ""Biologically inspired protection of deep networks from adversarial attacks"""
252,Domain Adaptation: Learning Bounds and Algorithms
253,Systematic evaluation of convolution neural network advances on the Imagenet
254,GENERALIZATION BOUNDS FOR DOMAIN ADAPTATION VIA DOMAIN TRANSFORMATIONS
255,Parseval Networks: Improving Robustness to Adversarial Examples
256,Deep Networks with Stochastic Depth
257,Categorical Reparameterization with Gumbel-Softmax
258,Smoothed Inference for Adversarially-Trained Models
259,Biologically inspired protection of deep networks from adversarial attacks
260,Delving into adversarial attacks on deep policies
261,Analysis of classifiers’ robustness to adversarial perturbations
262,signSGD: compressed optimisation for non-convex problems
263,"Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"
264,On Calibration of Modern Neural Networks
265,The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)
266,MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems
267,A Course in Game Theory
268,MagNet: A Two-Pronged Defense against Adversarial Examples
269,Improved Training of Wasserstein GANs
270,Early Methods for Detecting Adversarial Images
271,Task-Aware Compressed Sensing with Generative Adversarial Networks
272,Deep Learning Face Attributes in the Wild
273,Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms
274,Densely Connected Convolutional Networks
275,Nonlinear total variation based noise removal algorithms
276,The Split Bregman Method for L1-Regularized Problems
277,Adversary Resistant Deep Neural Networks with an Application to Malware Detection
278,The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables
279,Nonlocally Centralized Sparse Representation for Image Restoration
280,Is Deep Learning Safe for Robot Vision? Adversarial Examples Against the iCub Humanoid
281,Learning Adversary-Resistant Deep Neural Networks
282,"La cryptographie militaire (Auguste Kerckhoffs, 1883)"
283,"Show, Attend, Control, and Justify: Interpretable Learning for Self-Driving Cars"
284,"MagNet and ""Efficient Defenses Against Adversarial Attacks"" are Not Robust to Adversarial Examples"
285,Adversarial Example Defense: Ensembles of Weak Defenses are not Strong
286,Cascade Adversarial Machine Learning Regularized with a Unified Embedding
287,Wide Residual Networks
288,Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation
289,PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications
290,Learning representations by back-propagating errors
291,Learning long-term dependencies with gradient descent is difficult
292,Estimating Local Intrinsic Dimensionality
293,On the (Statistical) Detection of Adversarial Examples
294,Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN
295,The security of machine learning
296,DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket
297,Outside the Closed World: On Using Machine Learning for Network Intrusion Detection
298,Automatic analysis of malware behavior using machine learning
299,Deep neural network based malware detection using two dimensional binary program features
300,An evaluation of Naive Bayesian anti-spam filtering
301,Semantic Adversarial Examples
302,FeatureSmith: Automatically Engineering Features for Malware Detection by Mining the Security Literature
303,MARVIN: Efficient and Comprehensive Mobile App Classification through Static and Dynamic Analysis
304,Dense Associative Memory Is Robust to Adversarial Inputs
305,Using Non-invertible Data Transformations to Build Adversary-Resistant Deep Neural Networks
306,Automated Static Code Analysis for Classifying Android Applications Using Machine Learning
307,Efficient Detection of Zero-day Android Malware Using Normalized Bernoulli Naive Bayes
308,The Motivational Pull of Video Games: A Self-Determination Theory Approach
309,Race-Ethnic Differences in Factors Associated with Inhaled Steroid Adherence Among Adults with Asthma
310,"Activation of NK cells and T cells by NKG2D, a receptor for stress-inducible MICA."
311,Adversarially Regularized Autoencoders
312,Adversarially Regularized Autoencoders for Generating Discrete Structures
313,Variational Approaches for Auto-Encoding Generative Adversarial Networks
314,Improved Techniques for Training GANs
315,Towards Principled Methods for Training Generative Adversarial Networks
316,Do GANs actually learn the distribution? An empirical study
317,LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop
318,Wasserstein Auto-Encoders
319,Adversarial Examples for Evaluating Reading Comprehension Systems
320,DeepXplore: automated whitebox testing of deep learning systems
321,Toward Controlled Generation of Text
322,Edge-exchangeable graphs and sparsity (NIPS 2016)
323,Synthetic and Natural Noise Both Break Neural Machine Translation
324,Stochastic Gradient VB and the Variational Auto-Encoder
325,Understanding Neural Networks through Representation Erasure
326,Enhanced LSTM for Natural Language Inference
327,A causal framework for explaining the predictions of black-box sequence-to-sequence models
328,Anchors: High-Precision Model-Agnostic Explanations
329,NIPS 2016 Tutorial: Generative Adversarial Networks
330,Provably Minimally-Distorted Adversarial Examples
331,Towards Robust Detection of Adversarial Examples
332,Ground-Truth Adversarial Examples
333,Machine Learning as an Adversarial Service: Learning Black-Box Adversarial Examples
334,Detecting Adversarial Attacks on Neural Network Policies with Visual Foresight
335,DeepDGA: Adversarially-Tuned Domain Generation and Detection
336,DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in Neural Networks
337,Adversarial vulnerability for any classifier
338,Exploring the space of adversarial images
339,Fundamental limits on adversarial robustness
340,SafetyNet: Detecting and Rejecting Adversarial Examples Robustly
341,Learning Universal Adversarial Perturbations with Generative Models
342,"Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks"
343,A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples
344,Adversarial Images for Variational Autoencoders
345,Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples
346,Towards Proving the Adversarial Robustness of Deep Neural Networks
347,Universal Adversarial Perturbations Against Semantic Image Segmentation
348,Foolbox: A Python toolbox to benchmark the robustness of machine learning models
349,Foolbox v0.8.0: A Python toolbox to benchmark the robustness of machine learning models
350,Adversarially Robust Generalization Requires More Data
351,Robustness to Adversarial Examples through an Ensemble of Specialists
352,Standard detectors aren't (currently) fooled by physical adversarial stop signs
353,Enhancing robustness of machine learning systems via data transformations
354,Multiple classifier systems for robust classifier design in adversarial environments
355,Evading Machine Learning Malware Detection
