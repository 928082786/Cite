,0
0,Intriguing properties of neural networks
1,Explaining and Harnessing Adversarial Examples
2,Towards Evaluating the Robustness of Neural Networks
3,Adversarial Machine Learning at Scale
4,SoK: Security and Privacy in Machine Learning
5,Ensemble Adversarial Training: Attacks and Defenses
6,Thermometer Encoding: One Hot Way To Resist Adversarial Examples
7,Stochastic Activation Pruning for Robust Adversarial Defense
8,Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models
9,Countering Adversarial Images using Input Transformations
10,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples
11,Adversarial Examples for Malware Detection
12,Generating Natural Adversarial Examples
13,Adversarial Examples: Attacks and Defenses for Deep Learning
14,Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey
15,DPATCH: An Adversarial Patch Attack on Object Detectors
16,Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples
17,Delving into Transferable Adversarial Examples and Black-box Attacks
18,Maxout Networks
19,Adversarial examples in the physical world
20,DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks
21,The Limitations of Deep Learning in Adversarial Settings
22,Adversarial Diversity and Hard Positive Generation
23,Universal Adversarial Perturbations
24,Adversarial Examples for Generative Models
25,Learning to Attack: Adversarial Transformation Networks
26,Adversarial Examples for Semantic Segmentation and Object Detection
27,ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models
28,One Pixel Attack for Fooling Deep Neural Networks
29,Boosting Adversarial Attacks with Momentum
30,Generating Adversarial Examples with Adversarial Networks
31,Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models
32,Spatially Transformed Adversarial Examples
33,Synthesizing Robust Adversarial Examples
34,Adversarial Risk and the Dangers of Evaluating Against Weak Attacks
35,Adversarial Attacks on Neural Network Policies
36,Adversarial Examples for Semantic Image Segmentation
37,Adversarial Attacks and Defences Competition
38,Wasserstein Generative Adversarial Networks
39,Robust Physical Adversarial Attack on Faster R-CNN Object Detector
40,Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser
41,Adversarial attacks on Copyright Detection Systems
42,Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists
43,Adversarial Attacks Against Medical Deep Learning Systems
44,Towards Deep Neural Network Architectures Robust to Adversarial Examples
45,Defensive Distillation is Not Robust to Adversarial Examples
46,Adversarial Logit Pairing
47,Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations
48,Towards the first adversarially robust neural network model on MNIST
49,Defense Against Adversarial Images Using Web-Scale Nearest-Neighbor Search
50,ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation
51,On Detecting Adversarial Perturbations
52,Adversarial and Clean Data Are Not Twins
53,Blocking Transferability of Adversarial Examples in Black-Box Learning Systems
54,Detecting Adversarial Samples from Artifacts
55,Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks
56,Understanding adversarial training: Increasing local stability of supervised models through robust optimization
57,Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations
58,Robustness May Be at Odds with Accuracy
59,Learning Perceptually-Aligned Representations via Adversarial Robustness
60,Adversarial Training and Robustness for Multiple Perturbations
61,Transfer of Adversarial Robustness Between Perturbation Types
62,A study of the effect of JPG compression on adversarial images
63,Provable defenses against adversarial examples via the convex outer adversarial polytope
64,Certified Defenses against Adversarial Examples
65,Extracting and composing robust features with denoising autoencoders
66,Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods
67,The Space of Transferable Adversarial Examples
68,On the Connection Between Adversarial Robustness and Saliency Map Interpretability
69,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness
70,On Evaluating Adversarial Robustness
71,Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer
72,Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks
73,Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality
74,Mitigating adversarial effects through randomization
75,PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples
76,Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition
77,The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions
78,Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses
79,Evaluating and Understanding the Robustness of Adversarial Logit Pairing
80,Robust Deep Learning via Reverse Cross-Entropy Training and Thresholding Test
81,Learning with a Strong Adversary
82,Adversarial Manipulation of Deep Representations
